#!/usr/bin/env python3
"""
æ‰‹åŠ¨æµ‹è¯•æ¨é€è„šæœ¬
"""

import sys
import os
from datetime import datetime, date
from pathlib import Path

# æ·»åŠ agentè·¯å¾„
sys.path.append('.')

from paperpulse.main import PaperPulseAgent
from paperpulse.supabase_client import SupabaseClient
from paperpulse.arxiv_client import ArxivClient
from dotenv import load_dotenv

def manual_push_test():
    """æ‰‹åŠ¨æµ‹è¯•æ¨é€"""
    load_dotenv('.env')
    
    print("ğŸš€ æ‰‹åŠ¨æµ‹è¯•æ¨é€æµç¨‹")
    print("=" * 50)
    
    try:
        # 1. æµ‹è¯•æ›´å¹¿æ³›çš„å…³é”®è¯æœç´¢
        print("\nğŸ” 1. æµ‹è¯•å…³é”®è¯æœç´¢...")
        client = ArxivClient()
        
        # ä½¿ç”¨æ›´å¹¿æ³›çš„å…³é”®è¯
        test_keywords = ['transformer', 'attention', 'neural', 'AI']
        all_papers = []
        
        for keyword in test_keywords:
            papers = client.search_papers(keyword, days_back=7)  # æ‰©å±•åˆ°7å¤©
            print(f"   {keyword}: {len(papers)} ç¯‡è®ºæ–‡")
            all_papers.extend(papers)
        
        # å»é‡
        unique_papers = []
        seen_ids = set()
        for paper in all_papers:
            if paper['id'] not in seen_ids:
                seen_ids.add(paper['id'])
                unique_papers.append(paper)
        
        print(f"   ğŸ“š æ€»å…±æ‰¾åˆ° {len(unique_papers)} ç¯‡å”¯ä¸€è®ºæ–‡")
        
        if len(unique_papers) > 0:
            print("\nğŸ“„ å‰3ç¯‡è®ºæ–‡:")
            for i, paper in enumerate(unique_papers[:3]):
                print(f"   {i+1}. {paper['title']}")
                print(f"      å‘å¸ƒ: {paper['published']}")
        
        # 2. ç”Ÿæˆå®Œæ•´çš„digestæ–‡ä»¶
        print(f"\nğŸ’¾ 2. ç”Ÿæˆdigestæ–‡ä»¶...")
        agent = PaperPulseAgent()
        
        # å–å‰20ç¯‡è®ºæ–‡
        papers_to_save = unique_papers[:20]
        success = agent.save_daily_digest(papers_to_save)
        
        print(f"   ğŸ“ ä¿å­˜ç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        print(f"   ğŸ“„ ä¿å­˜è®ºæ–‡æ•°: {len(papers_to_save)}")
        
        # 3. æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
        digest_file = Path("../web/public/static/digests") / f"{date.today().isoformat()}.json"
        if digest_file.exists():
            print(f"   ğŸ“ æ–‡ä»¶è·¯å¾„: {digest_file}")
            print(f"   ğŸ“ æ–‡ä»¶å¤§å°: {digest_file.stat().st_size} å­—èŠ‚")
            
            # éªŒè¯JSONæ ¼å¼
            import json
            try:
                with open(digest_file, 'r') as f:
                    data = json.load(f)
                print(f"   âœ… JSONæ ¼å¼æ­£ç¡®ï¼ŒåŒ…å« {len(data.get('papers', []))} ç¯‡è®ºæ–‡")
            except Exception as e:
                print(f"   âŒ JSONæ ¼å¼é”™è¯¯: {e}")
        
        # 4. æµ‹è¯•è®¢é˜…è€…å¤„ç†
        print(f"\nğŸ‘¥ 3. æµ‹è¯•è®¢é˜…è€…å¤„ç†...")
        subscribers = agent.load_subscribers()
        print(f"   ğŸ“§ æ‰¾åˆ° {len(subscribers)} ä½è®¢é˜…è€…")
        
        for subscriber in subscribers:
            print(f"   å¤„ç†: {subscriber.email}")
            print(f"   å…³é”®è¯: {subscriber.keywords}")
            
            # è¿‡æ»¤ç›¸å…³è®ºæ–‡
            relevant_papers = []
            for paper in papers_to_save:
                for keyword in subscriber.keywords:
                    if keyword.lower() in paper['title'].lower() or keyword.lower() in paper['abstract'].lower():
                        relevant_papers.append(paper)
                        break
            
            print(f"   ç›¸å…³è®ºæ–‡: {len(relevant_papers)}")
            
            if len(relevant_papers) > 0:
                print(f"   ğŸ“§ æ¨¡æ‹Ÿå‘é€é‚®ä»¶ç»™ {subscriber.email}")
                print(f"   ğŸ“„ åŒ…å« {len(relevant_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
            else:
                print(f"   âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡")
        
        print(f"\nâœ… æ‰‹åŠ¨æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    manual_push_test() 